---
layout: post
title: Animatable Nerf Preparation
excerpt_separator: <!--more-->
---
****
<img class="post-thumbnail" src="/images/ani_nerf_2D.gif">
A framework for developing and comparing volume deformation models, visualized with a raymarching camera. These models can then be translated to use for real-time animated NeRFs.
<!--more-->
<h2 class="clear">The Problem</h2>
<p>I want to decouple NeRF rendering from deformation to create a framework for animatable NeRFs. This would allow training a model on arbitrary volume deformations, rather than relying on input videos or physics simulations, to view the NeRF as if it were deformed. Implementation requires addressing several factors within the render pipeline.</p>
<h3>Rendering a NeRF</h3>
<p>The main step to a NeRF render is raymarching. Cast rays from the camera into a field, sample the NeRF model along these rays, and composite the samples into a final image. And if I want to deform this NeRF, I can represent it with two separate volumes. A "deformed" volume, where the camera resides, and a "canonical" volume, where the static NeRF resides. The camera casts straight rays through the deformed volume, since the resulting deformed NeRF is what I want to view for a given instance of the animation. Then in order to get the density and color of a NeRF at a certain ray sample in the deformed volume, I need a model/method to correlate these samples back to a point in the canonical volume. I will be referring this correlation back to the canonical volume as the "inverse deformation". In essence, I map straight rays from the deformed volume to learned, curved rays within the canonical volume. These curved rays sample the static NeRF, relaying its data to the correlated camera sample in order to render a deformed NeRF.</p>
<p>A fundementally simple problem that gets substatially more complex when real-time performance is a factor. Some key features for real-time performance of static NeRFs are found in InstantNGP's NeRF implementation: a faster model (compared to previous NeRF implementations), and a volumetric "mask" via a binary voxel-based occupancy grid (determines if the NeRF occupies a particular region in space). The first feature is straightforward to translate to a non-static NeRF, but the occupancy grid is where things could get out of hand. </p>

<h3>Where is the NeRF?</h3>
<p>Obviously, an occupancy grid works fine if the NeRF is static, but it will fail to funtion once the NeRF deforms. As the NeRF is no longer where the occupancy grid expects it to be. The inverse deformation model could be used at every sample until you reach the static occupancy grid in the canonical volume, but this largely ignores the speed-up of using the occupancy grid as a sampling mask. So why not just create another occupancy grid for the deformed volume? Caching occupancies for all deformed frames can definitely work for a sigular deformation or even a short time-tracked animation. All I would need to do is define a frame index I want to render, raymarch with that frame's deformed occupancy grid, and only sample the inverse deformation model and the NeRF when rays pass through this deformed occupancy grid. However, what about expanding past fixed framecount animations? Such as animating based on a combination of various parameters. Anywhere from pose animation with a skeleton to face animation via blendshapes to secondary effects such as muscle or hair simulation based on the more strict parameters. A 128^3 binary occupancy grid may only be 0.2 megabytes, but pre calculating and storing millions of these to handle every scenario would be a poor use of computation power and memory.</p>
<p>So the problem becomes: how can this be avoided? One strategy is to use a simple, direct point-to-point model for the inverse deformation correlation, while a separate, complex model handles predicting the required occupancy grid in real-time. It is an interesting approach that I would like to test in the future, but the implementation I took for this project takes a different approach.</p>

<h2>Implementation of the Inverse Deformation Model</h2>
<p>
Theres two aspects I want to preserve in a real-time focused implementation of an animatable NeRF: keep an occupancy grid for sample masking, and prevent multiple inverse deformation model samples per ray before the occupancy grid is reached. Meaning, I wanted a way to get the resulting full deformed ray with one pass of the inverse deformation model, allowing for much quicker sampling before the occupancy grid is reached. No going between the CPU and GPU after each inverse deformation model sample to see which points should start sampling from the NeRF model and which are still in empty space. Knowing that the ray sampling in the canonical space is a warped/curved version of the ray in the deformed volume, I figured replicating that curve with parametric curves is the way to go.
</p>
<h3>Model Progression</h3>
<p>
Before Hopping directly into a parametric curve implementation, I worked my way through more complex/abstract models. In order of testing, they are:
</p>

<h4>1. Direct Point Correlation Model</h4>
   - For any point (<em>x</em>, <em>y</em>) in the deformed volume, output the point (<em>x'</em>, <em>y'</em>) that correlates to the correct point in the canonical volume
<h4>2. Point Offsets Model</h4>
   - For any point (<em>x</em>, <em>y</em>) in the deformed volume, output the offsets (<em>i</em>, <em>j</em>),that makes (<em>x + i</em>, <em>y + j</em>) correlate to the correct point in the canonical volume
<h4>3. Ray Origin Model</h4>
   - For a given origin point of a camera ray (<em>x</em>, <em>y</em>) and <em>t</em> for distance along the ray, output the offsets (<em>i</em>, <em>j</em>), that makes (<em>x + i</em>, <em>y + j</em>) correlate to the correct point in the canonical volume
<h4>4. Batch Ray Origin Model</h4>
   - For a given origin point of a camera ray (<em>x</em>, <em>y</em>), output an array of offsets along the entire ray <<em>i</em>, <em>j</em>>, that makes each (<em>x + i</em>, <em>y + j</em>) at a given <em>t</em> distace along the ray correlate to the correct points in the canonical volume
<h4>5. Akima Spline Model</h4>
   - For a given origin point of a camera ray (<em>x</em>, <em>y</em>) and ray angle , output a set of akima spline knot parameters <<em>i</em>, <em>j</em>, <em>t</em>>to define parameterized offset splines for each axis: <<em>i</em>, <em>t</em>> for the <em>x</em> axis, and <<em>j</em>, <em>t</em>> for the <em>y</em> axis. The predicted <em>t</em> values are shared between the <em>x</em> axis spline and <em>y</em> axis spline. Use the <em>t</em> value for the current camera ray sample to perform the standard functions for an Akima spline and combine with the camera sample (<em>x</em>, <em>y</em>) to get the corralated point in the canonical volume
<h4>6. Entry/Exit Akima Spline Model</h4>
   - For a given entry point into the deformed volume's bounding box (<em>x_entry</em>, <em>y_entry</em>) and a given exit point of the deformed volume's bounding box (<em>x_exit</em>, <em>y_exit</em>), output the same splines as the Akima Spline Model

<h2>Training Environment</h2>
<img style="float:center;margin: 10x 0px 10px 0px;" src="/images/ani_nerf_2D_2.gif">

<p>The majority of the values (deformation parameters, camera position/rotation/resolution) are configureable. The gif above shows how the ray count can be changed on the fly. In fact, since the inverse deformation model only considers one ray in its model size for input/output, you can change the "resolution" without retraining the model</p>
<p>The visualization for the environment is: </p>

- The red area is the canonical space
- The green area is the deformed space
- The black dots represent the canonical object when in the canonical space and the correlated deformed points in the deformed space
- The yellow rays represent the camera rays
- The pink dots are the spline knots
- The red dots are the predicted values (calculated using the spline knots)
- The blue dots are the ground truth samples

<p>The training data is a jittered sample of camera positions along an orbiting path</p>

<h2>Results</h2>
<img style="float:center;margin: 10x 0px 10px 0px;" src="/images/ani_nerf_2D.gif">
<p>
The model does a decent job tracking the deformation throughout a camera orbit, but there are some noticable points of improvement. Of course, overall accuracy could improve, but also there is a "wobble" between very small camera movements. I have tried various loss calculation improvements, such as taking into account ray smoothness, ray neighbor consistency, and ray reprojection straightness. They have not resulted in significant improvements for soley the orbit, but more testing needs to be done to find the optimal weightings for these loss funtions.
Also, there was not a significant difference between using the camera ray origins versus using the entry/exit points, however I believe this is mostly due to the training data being limited to a mostly-fixed distance orbit around the object. Entry/exit should help the model generalize with free camera movement.
</p>
